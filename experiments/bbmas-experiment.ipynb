{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:36:35.924101Z",
     "iopub.status.busy": "2024-12-06T14:36:35.923629Z",
     "iopub.status.idle": "2024-12-06T14:36:48.826162Z",
     "shell.execute_reply": "2024-12-06T14:36:48.824636Z",
     "shell.execute_reply.started": "2024-12-06T14:36:35.924043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %pip install imblearn xgboost scikit-learn numpy pandas matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:36:48.829654Z",
     "iopub.status.busy": "2024-12-06T14:36:48.829086Z",
     "iopub.status.idle": "2024-12-06T14:36:50.692297Z",
     "shell.execute_reply": "2024-12-06T14:36:50.690696Z",
     "shell.execute_reply.started": "2024-12-06T14:36:48.829602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, precision_score, f1_score, accuracy_score, roc_auc_score, recall_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:36:50.694641Z",
     "iopub.status.busy": "2024-12-06T14:36:50.693945Z",
     "iopub.status.idle": "2024-12-06T14:36:57.991040Z",
     "shell.execute_reply": "2024-12-06T14:36:57.989572Z",
     "shell.execute_reply.started": "2024-12-06T14:36:50.694586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('bb-mas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:36:58.155163Z",
     "iopub.status.busy": "2024-12-06T14:36:58.154695Z",
     "iopub.status.idle": "2024-12-06T14:36:58.162840Z",
     "shell.execute_reply": "2024-12-06T14:36:58.161602Z",
     "shell.execute_reply.started": "2024-12-06T14:36:58.155116Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 113\n",
      "Number of samples: 2545\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of users: {len(data['UID'].unique())}\")\n",
    "print(f\"Number of samples: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:36:58.164867Z",
     "iopub.status.busy": "2024-12-06T14:36:58.164390Z",
     "iopub.status.idle": "2024-12-06T14:36:58.194958Z",
     "shell.execute_reply": "2024-12-06T14:36:58.193301Z",
     "shell.execute_reply.started": "2024-12-06T14:36:58.164814Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>SID</th>\n",
       "      <th>capslock_usage</th>\n",
       "      <th>negative_ud</th>\n",
       "      <th>negative_uu</th>\n",
       "      <th>rsa_ratio</th>\n",
       "      <th>lsa_ratio</th>\n",
       "      <th>mean_hold_time</th>\n",
       "      <th>mean_f1</th>\n",
       "      <th>mean_f2</th>\n",
       "      <th>mean_f3</th>\n",
       "      <th>mean_f4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>2.251130</td>\n",
       "      <td>2.586130</td>\n",
       "      <td>2.581304</td>\n",
       "      <td>2.916304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.156931</td>\n",
       "      <td>0.820905</td>\n",
       "      <td>0.977836</td>\n",
       "      <td>0.977164</td>\n",
       "      <td>1.134095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203722</td>\n",
       "      <td>2.014389</td>\n",
       "      <td>2.218111</td>\n",
       "      <td>2.221241</td>\n",
       "      <td>2.424963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.864303</td>\n",
       "      <td>-0.659788</td>\n",
       "      <td>1.204515</td>\n",
       "      <td>1.198258</td>\n",
       "      <td>3.062561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.197802</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.897033</td>\n",
       "      <td>-1.758824</td>\n",
       "      <td>0.138209</td>\n",
       "      <td>0.659181</td>\n",
       "      <td>2.556214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UID  SID  capslock_usage  negative_ud  negative_uu  rsa_ratio  lsa_ratio  \\\n",
       "0    1    1               0     0.152174     0.043478        1.0        0.0   \n",
       "1    1    2               0     0.068966     0.017241        1.0        0.0   \n",
       "2    1    3               0     0.129630     0.037037        1.0        0.0   \n",
       "3    1    4               0     0.136364     0.045455        1.0        0.0   \n",
       "4    1    5               0     0.197802     0.065934        1.0        0.0   \n",
       "\n",
       "   mean_hold_time   mean_f1   mean_f2   mean_f3   mean_f4  \n",
       "0        0.335000  2.251130  2.586130  2.581304  2.916304  \n",
       "1        0.156931  0.820905  0.977836  0.977164  1.134095  \n",
       "2        0.203722  2.014389  2.218111  2.221241  2.424963  \n",
       "3        1.864303 -0.659788  1.204515  1.198258  3.062561  \n",
       "4        1.897033 -1.758824  0.138209  0.659181  2.556214  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "### Data Description\n",
    "\n",
    "This dataset is transformed from the BB-MAS keystrokes dataset. Each participant joined two session:\n",
    "- Fixed-text: the participants typed the same texts\n",
    "- Free-text: the participants typed a random long predefined texts\n",
    "\n",
    "Their keystroke events is recorded with three columns:\n",
    "- **key** - the name of the pressed key\n",
    "- **direction** - indicates if the key is pressed or released (0 | 1)\n",
    "- **timestamp** - the time that the event happens, it is UNIX timetamp in milisecons\n",
    "\n",
    "Our datasets divided the events into samples with the duration 2 minutes for each samples. Some features are extracted from the keystroke events:\n",
    "- **Hold time**: the duration when user pressed then released the key (also known as Dwell Time)\n",
    "- **F1: Press-to-Press Time**: The time interval between the press of one key and the press of the next key.\n",
    "- **F2: Release-to-Press Time**: The time interval between the release of one key and the press of the next key.\n",
    "- **F3: Press-to-Release Time**: The time interval a key is held down (from press to release).\n",
    "- **F4: Release-to-Release Time**: The time interval between the release of one key and the release of the next key.\n",
    "\n",
    "\n",
    "### Experiement Description\n",
    "\n",
    "- Interate through the user samples, 1 user is picked as legitimate, the remaining ones will be the imposters.\n",
    "- Through 31 samples, we want to proved the efficicency of our method to dynamically train keystroke dynamics recognition models for each user.\n",
    "- In real-world use cases, the data of imposter always more than the legitimate one (at enrollment time), so we will apply SMOTE for imbalance datasets top prevent the bias on majority class (imposter).\n",
    "- Extreme Gradient Booster (XGB) is used in this experiement as it was proved as efficient models with high performance (comparing to Random Forest) in this area.\n",
    "- We used GridSearch for retrieving the best performance model to use for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define needed utilities for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(\n",
    "    df,\n",
    "    factor=5,\n",
    "    perturbation_range=(-0.001, 0.001),\n",
    "    columns=None,\n",
    "):\n",
    "    augmented_data = []\n",
    "    columns = columns or df.columns\n",
    "\n",
    "    # Generate augmented data\n",
    "    for _ in range(factor):\n",
    "        for _, row in df.iterrows():\n",
    "            synthetic_sample = row.copy()  # Start with a copy of the original row\n",
    "\n",
    "            for column in columns:\n",
    "                if column in df.columns:\n",
    "                    # Randomly perturb the column within the specified range\n",
    "                    perturbation = np.random.uniform(*perturbation_range)\n",
    "                    synthetic_sample[column] += perturbation\n",
    "\n",
    "            augmented_data.append(synthetic_sample)\n",
    "\n",
    "    # Create a DataFrame from the augmented data\n",
    "    df_augmented = pd.DataFrame(augmented_data)\n",
    "\n",
    "    # Combine original and augmented data\n",
    "    df_combined = pd.concat([df, df_augmented], ignore_index=True)\n",
    "    return df_combined, df_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:36:58.208715Z",
     "iopub.status.busy": "2024-12-06T14:36:58.208359Z",
     "iopub.status.idle": "2024-12-06T14:36:58.218697Z",
     "shell.execute_reply": "2024-12-06T14:36:58.217543Z",
     "shell.execute_reply.started": "2024-12-06T14:36:58.208683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_pipeline():\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\n",
    "                \"xgb\",\n",
    "                XGBClassifier(eval_metric=\"logloss\"),\n",
    "            ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:36:58.222598Z",
     "iopub.status.busy": "2024-12-06T14:36:58.222179Z",
     "iopub.status.idle": "2024-12-06T14:36:58.243620Z",
     "shell.execute_reply": "2024-12-06T14:36:58.242400Z",
     "shell.execute_reply.started": "2024-12-06T14:36:58.222564Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def trainer(user_id):\n",
    "    features = [\n",
    "        'mean_f1', 'mean_f2', 'mean_f3', 'mean_f4', \n",
    "        'mean_hold_time', 'capslock_usage', 'negative_uu', 'negative_ud'\n",
    "    ]\n",
    "\n",
    "    # Separate legitimate and imposter data\n",
    "    imposter_data = data[data[\"UID\"] != user_id].copy()[features]\n",
    "    legitimate_data = data[data[\"UID\"] == user_id].copy()[features]\n",
    "\n",
    "    # Split legitimate data into train and test\n",
    "    legitimate_data_for_train = legitimate_data.sample(n=5)\n",
    "    legitimate_data_for_test = legitimate_data.drop(legitimate_data_for_train.index)\n",
    "\n",
    "    # Perform data augmentation on legitimate training data\n",
    "    legitimate_data, _ = data_augmentation(\n",
    "        legitimate_data_for_train,\n",
    "        factor=4,\n",
    "        columns=[\n",
    "            \"mean_hold_time\", \"mean_f1\", \"mean_f2\", \"mean_f3\", \"mean_f4\",\n",
    "        ],\n",
    "        perturbation_range=(-0.02, 0.02),\n",
    "    )\n",
    "\n",
    "    imposter_data.loc[:, 'label'] = 0\n",
    "    legitimate_data.loc[:, 'label'] = 1\n",
    "    legitimate_data_for_test.loc[:, 'label'] = 1\n",
    "\n",
    "    train_legitimate, test_legitimate = train_test_split(\n",
    "        legitimate_data, test_size=0.2\n",
    "    )\n",
    "    train_imposter, test_imposter = train_test_split(\n",
    "        imposter_data, test_size=0.2\n",
    "    )\n",
    "\n",
    "\n",
    "    # Combine training and testing datasets\n",
    "    train_set = pd.concat([train_legitimate, train_imposter])\n",
    "    test_set = pd.concat([test_legitimate ,test_imposter, legitimate_data_for_test])\n",
    "\n",
    "    # Shuffle datasets\n",
    "    train_set = train_set.sample(frac=1).reset_index(drop=True)\n",
    "    test_set = test_set.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Separate features and labels\n",
    "    x_train = train_set.drop(columns=[\"label\"])\n",
    "    x_test = test_set.drop(columns=[\"label\"])\n",
    "    y_train = train_set[\"label\"]\n",
    "    y_test = test_set[\"label\"]\n",
    "\n",
    "    # Handle class imbalance using SMOTE\n",
    "    smote = SMOTE(k_neighbors=4)\n",
    "    x_train_balanced, y_train_balanced = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "    # Define parameter grid for GridSearch\n",
    "    param_grid = {\n",
    "        \"xgb__n_estimators\": [50, 100, 200],\n",
    "        \"xgb__max_depth\": [3, 5, 7],\n",
    "        \"xgb__learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"xgb__subsample\": [0.8, 1.0],\n",
    "        \"xgb__scale_pos_weight\": [25, 50, 75, 99, 100],\n",
    "    }\n",
    "\n",
    "    # Create a pipeline and perform GridSearch\n",
    "    pipeline = create_pipeline()\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "        verbose=0,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(x_train_balanced, y_train_balanced)\n",
    "\n",
    "    # Get the best model and predictions\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_prob = best_model.predict_proba(x_test)[:, 1]\n",
    "    y_pred = best_model.predict(x_test)\n",
    "\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "\n",
    "    # Calculate EER (point where FAR == FRR)\n",
    "    fnr = 1 - tpr  # False Negative Rate\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fpr - fnr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fpr - fnr))]\n",
    "\n",
    "    # Calculate AUC-ROC\n",
    "    auc_roc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    # Return all metrics\n",
    "    return {\n",
    "        \"best_model\": best_model,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"f1_score\": f1_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred, labels=[0, 1]),\n",
    "        \"recall_score\": recall_score(y_test, y_pred),\n",
    "        \"eer_threshold\": eer_threshold,\n",
    "        \"eer\": eer,\n",
    "        \"auc_roc\": auc_roc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T14:36:58.245390Z",
     "iopub.status.busy": "2024-12-06T14:36:58.244984Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Users:   0%|          | 0/113 [00:00<?, ?it/s]/Users/khoadtran/Documents/dev/research/lab/.venv/lib/python3.9/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "Processing Users:   8%|▊         | 9/113 [01:37<18:52, 10.89s/it]/Users/khoadtran/Documents/dev/research/lab/.venv/lib/python3.9/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "Processing Users:  15%|█▌        | 17/113 [02:57<15:55,  9.96s/it]/Users/khoadtran/Documents/dev/research/lab/.venv/lib/python3.9/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "Processing Users:  55%|█████▍    | 62/113 [10:32<09:06, 10.71s/it]/Users/khoadtran/Documents/dev/research/lab/.venv/lib/python3.9/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "Processing Users:  58%|█████▊    | 65/113 [11:05<08:40, 10.85s/it]/Users/khoadtran/Documents/dev/research/lab/.venv/lib/python3.9/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "Processing Users:  58%|█████▊    | 66/113 [11:15<08:20, 10.64s/it]/Users/khoadtran/Documents/dev/research/lab/.venv/lib/python3.9/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "Processing Users:  72%|███████▏  | 81/113 [13:54<05:57, 11.17s/it]/Users/khoadtran/Documents/dev/research/lab/.venv/lib/python3.9/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "Processing Users:  95%|█████████▍| 107/113 [18:08<00:57,  9.53s/it]/Users/khoadtran/Documents/dev/research/lab/.venv/lib/python3.9/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "Processing Users:  98%|█████████▊| 111/113 [18:47<00:19,  9.85s/it]/Users/khoadtran/Documents/dev/research/lab/.venv/lib/python3.9/site-packages/numpy/ma/core.py:2846: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "Processing Users: 100%|██████████| 113/113 [19:04<00:00, 10.13s/it]\n"
     ]
    }
   ],
   "source": [
    "history = {}\n",
    "\n",
    "# Use tqdm to show progress bar\n",
    "for user_id in tqdm(data['UID'].unique(), desc=\"Processing Users\"):\n",
    "    try:\n",
    "        results = trainer(user_id)\n",
    "        \n",
    "        history[user_id] = {\n",
    "            'accuracy': results['accuracy'],\n",
    "            'f1': results['f1_score'],\n",
    "            'precision': results['precision'],\n",
    "            'confusion_matrix': results['confusion_matrix'],\n",
    "            'eer': results['eer'],\n",
    "            'eer_threshold': results['eer_threshold'],\n",
    "            'auc_roc': results['auc_roc']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(user_id)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('bbmas_histories.pkl', 'wb') as file:\n",
    "    pickle.dump(history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.963, Std Accuracy: 0.007\n",
      "Mean F1: 0.391, Std F1: 0.102\n",
      "Mean Precision: 0.658, Std Precision: 0.156\n"
     ]
    }
   ],
   "source": [
    "# Extract the metrics\n",
    "accuracies = [data['accuracy'] for data in history.values()]\n",
    "f1_scores = [data['f1'] for data in history.values()]\n",
    "precisions = [data['precision'] for data in history.values()]\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "std_f1 = np.std(f1_scores)\n",
    "\n",
    "mean_precision = np.mean(precisions)\n",
    "std_precision = np.std(precisions)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Mean Accuracy: {mean_accuracy:.3f}, Std Accuracy: {std_accuracy:.3f}\")\n",
    "print(f\"Mean F1: {mean_f1:.3f}, Std F1: {std_f1:.3f}\")\n",
    "print(f\"Mean Precision: {mean_precision:.3f}, Std Precision: {std_precision:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6235290,
     "sourceId": 10108207,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
